{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import urllib, urllib.request\n",
    "import feedparser\n",
    "from datetime import date\n",
    "from xml.etree import ElementTree\n",
    "import time\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "\n",
    "BASE = 'http://export.arxiv.org/api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query='a',\n",
    "    max_results = 300000,\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "# for result in search.results():\n",
    "#     print(result.title, result.published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for result in search.results():\n",
    "    results.append(result)\n",
    "    if len(results) == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(search.results())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://export.arxiv.org/api/query?search_query=all:a+OR+all:the'\n",
    "url = 'http://export.arxiv.org/oai2?verb=ListRecords&set=math&from=2015-01-01&until=2015-01-31&metadataPrefix=arXiv'\n",
    "data = urllib.request.urlopen(url)\n",
    "#print(data.read().decode('utf-8'))\n",
    "parsed = feedparser.parse(data)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test = urllib.request.urlopen('http://export.arxiv.org/oai2?verb=ListRecords&set=math&from=2015-01-01&until=2015-01-31&metadataPrefix=arXiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedparser.parse(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get math stuff from 2015\n",
    "# response = requests.get('http://export.arxiv.org/oai2?verb=ListRecords&set=math&from=2015-01-01&until=2015-01-31&metadataPrefix=arXiv')\n",
    "\n",
    "# get EVERYTHING\n",
    "# response = requests.get('http://export.arxiv.org/oai2?verb=ListRecords&set=math&metadataPrefix=arXiv&until=2007-05-25')\n",
    "# print(response.text)\n",
    "# response2 = requests.get('http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=5807382|38001')\n",
    "# print(response2.text)\n",
    "dict = xmltodict.parse(response2.text)\n",
    "if dict['OAI-PMH']['ListRecords']['resumptionToken'].get('#text'):\n",
    "    print('Still more')\n",
    "else:\n",
    "    print('Done!')\n",
    "\n",
    "dict1 = xmltodict.parse(response.text)\n",
    "dict1.update(dict)\n",
    "# dict = json.loads(json.dumps(dict))\n",
    "type(dict['OAI-PMH']['ListRecords']['record'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5807359|1001'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(response.text)\n",
    "# response2 = requests.get('http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=5807359|1001')\n",
    "# print(response2.text)\n",
    "dict = xmltodict.parse(response.text)\n",
    "dict['OAI-PMH']['ListRecords']['resumptionToken']['#text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@xmlns': 'http://arxiv.org/OAI/arXiv/',\n",
       " '@xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
       " '@xsi:schemaLocation': 'http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd',\n",
       " 'id': '0704.0001',\n",
       " 'created': '2007-04-02',\n",
       " 'updated': '2007-07-24',\n",
       " 'authors': {'author': [{'keyname': 'Bal√°zs', 'forenames': 'C.'},\n",
       "   {'keyname': 'Berger', 'forenames': 'E. L.'},\n",
       "   {'keyname': 'Nadolsky', 'forenames': 'P. M.'},\n",
       "   {'keyname': 'Yuan', 'forenames': 'C. -P.'}]},\n",
       " 'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
       " 'categories': 'hep-ph',\n",
       " 'comments': '37 pages, 15 figures; published version',\n",
       " 'report-no': 'ANL-HEP-PR-07-12',\n",
       " 'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
       " 'doi': '10.1103/PhysRevD.76.013009',\n",
       " 'abstract': 'A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for processing giant dict at very end\n",
    "\n",
    "import json\n",
    "dict = json.loads(json.dumps(dict))\n",
    "dict['OAI-PMH']['ListRecords']['resumptionToken']['#text']\n",
    "# dict['OAI-PMH']['resumptionToken']['#text']\n",
    "records = dict['OAI-PMH']['ListRecords']['record']\n",
    "df1 = pd.DataFrame(records)\n",
    "df2 = pd.DataFrame(list(df1['metadata']))\n",
    "#records\n",
    "df2\n",
    "records\n",
    "df1['metadata'][0]['arXiv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2022-03-03'\n",
    "start = 0\n",
    "max = 100\n",
    "papers = []\n",
    "math_query = \"\"\"\n",
    "cat:math.ac+OR+cat:math.ag+OR+cat:math.ap+OR+cat:math.at+OR+cat:math.ca+OR+cat:math.co+OR+\n",
    "cat:math.ct+OR+cat:math.cv+OR+cat:math.dg+OR+cat:math.ds+OR+cat:math.fa+OR+cat:math.gm+OR+\n",
    "cat:math.gn+OR+cat:math.gr+OR+cat:math.gt+OR+cat:math.ho+OR+cat:math.it+OR+cat:math.kt+OR+\n",
    "cat:math.lo+OR+cat:math.mg+OR+cat:math.mp+OR+cat:math.na+OR+cat:math.nt+OR+cat:math.oa+OR+\n",
    "cat:math.oc+OR+cat:math.pr+OR+cat:math.qa+OR+cat:math.ra+OR+cat:math.rt+OR+cat:math.sg+OR+\n",
    "cat:math.sp+OR+cat:math.st\n",
    "\"\"\".replace('\\n', '')\n",
    "\n",
    "while (date == '2022-03-03'):\n",
    "    print(f'Results {start} - ' + str(start + max))\n",
    "    query = f'/query?search_query={math_query}&start={start}&max_results={max}&sortBy=submittedDate'\n",
    "\n",
    "    res = data = urllib.request.urlopen(BASE + query)\n",
    "    parsed = feedparser.parse(res)\n",
    "\n",
    "    for entry in parsed.entries:\n",
    "        paper = {}\n",
    "        paper['date'] = entry.published\n",
    "        paper['title'] = entry.title\n",
    "        paper['first_author'] = entry.author\n",
    "        paper['summary'] = entry.summary\n",
    "        paper['primary_category'] = entry.arxiv_primary_category['term']\n",
    "        papers.append(paper)\n",
    "        date = paper['date'][0:10]\n",
    "\n",
    "    start += max\n",
    "\n",
    "\n",
    "    if not papers:\n",
    "        break\n",
    "\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(papers))\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = f'/query?search_query={fuck_me}&start=0&max_results=10&sortBy=submittedDate'\n",
    "# res = data = urllib.request.urlopen(BASE + query)\n",
    "# parsed = feedparser.parse(res)\n",
    "parsed.entries[0].arxiv_primary_category['term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This sample script illustrates a basic arXiv api call\n",
    "followed by parsing of the results using the \n",
    "feedparser python module.\n",
    "\n",
    "Please see the documentation at \n",
    "http://export.arxiv.org/api_help/docs/user-manual.html\n",
    "for more information, or email the arXiv api \n",
    "mailing list at arxiv-api@googlegroups.com.\n",
    "\n",
    "urllib is included in the standard python library.\n",
    "feedparser can be downloaded from http://feedparser.org/ .\n",
    "\n",
    "Author: Julius B. Lucks\n",
    "\n",
    "This is free software.  Feel free to do what you want\n",
    "with it, but please play nice with the arXiv API!\n",
    "\"\"\"\n",
    "\n",
    "import urllib\n",
    "import feedparser\n",
    "\n",
    "# Base api query url\n",
    "base_url = 'http://export.arxiv.org/api/query?';\n",
    "\n",
    "# Search parameters\n",
    "search_query = 'all:electron' # search for electron in all fields\n",
    "start = 0                     # retreive the first 5 results\n",
    "max_results = 5\n",
    "\n",
    "query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
    "                                                     start,\n",
    "                                                     max_results)\n",
    "\n",
    "# Opensearch metadata such as totalResults, startIndex, \n",
    "# and itemsPerPage live in the opensearch namespase.\n",
    "# Some entry metadata lives in the arXiv namespace.\n",
    "# This is a hack to expose both of these namespaces in\n",
    "# feedparser v4.1\n",
    "feedparser._FeedParserMixin.namespaces['http://a9.com/-/spec/opensearch/1.1/'] = 'opensearch'\n",
    "feedparser._FeedParserMixin.namespaces['http://arxiv.org/schemas/atom'] = 'arxiv'\n",
    "\n",
    "# perform a GET request using the base_url and query\n",
    "response = urllib.urlopen(base_url+query).read()\n",
    "\n",
    "# parse the response using feedparser\n",
    "feed = feedparser.parse(response)\n",
    "\n",
    "# print out feed information\n",
    "print ('Feed title: %s' % feed.feed.title)\n",
    "print 'Feed last updated: %s' % feed.feed.updated\n",
    "\n",
    "# print opensearch metadata\n",
    "print 'totalResults for this query: %s' % feed.feed.opensearch_totalresults\n",
    "print 'itemsPerPage for this query: %s' % feed.feed.opensearch_itemsperpage\n",
    "print 'startIndex for this query: %s'   % feed.feed.opensearch_startindex\n",
    "\n",
    "# Run through each entry, and print out information\n",
    "for entry in feed.entries:\n",
    "    print 'e-print metadata'\n",
    "    print 'arxiv-id: %s' % entry.id.split('/abs/')[-1]\n",
    "    print 'Published: %s' % entry.published\n",
    "    print 'Title:  %s' % entry.title\n",
    "    \n",
    "    # feedparser v4.1 only grabs the first author\n",
    "    author_string = entry.author\n",
    "    \n",
    "    # grab the affiliation in <arxiv:affiliation> if present\n",
    "    # - this will only grab the first affiliation encountered\n",
    "    #   (the first affiliation for the first author)\n",
    "    # Please email the list with a way to get all of this information!\n",
    "    try:\n",
    "        author_string += ' (%s)' % entry.arxiv_affiliation\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    print 'Last Author:  %s' % author_string\n",
    "    \n",
    "    # feedparser v5.0.1 correctly handles multiple authors, print them all\n",
    "    try:\n",
    "        print 'Authors:  %s' % ', '.join(author.name for author in entry.authors)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    # get the links to the abs page and pdf for this e-print\n",
    "    for link in entry.links:\n",
    "        if link.rel == 'alternate':\n",
    "            print 'abs page link: %s' % link.href\n",
    "        elif link.title == 'pdf':\n",
    "            print 'pdf link: %s' % link.href\n",
    "    \n",
    "    # The journal reference, comments and primary_category sections live under \n",
    "    # the arxiv namespace\n",
    "    try:\n",
    "        journal_ref = entry.arxiv_journal_ref\n",
    "    except AttributeError:\n",
    "        journal_ref = 'No journal ref found'\n",
    "    print 'Journal reference: %s' % journal_ref\n",
    "    \n",
    "    try:\n",
    "        comment = entry.arxiv_comment\n",
    "    except AttributeError:\n",
    "        comment = 'No comment found'\n",
    "    print 'Comments: %s' % comment\n",
    "    \n",
    "    # Since the <arxiv:primary_category> element has no data, only\n",
    "    # attributes, feedparser does not store anything inside\n",
    "    # entry.arxiv_primary_category\n",
    "    # This is a dirty hack to get the primary_category, just take the\n",
    "    # first element in entry.tags.  If anyone knows a better way to do\n",
    "    # this, please email the list!\n",
    "    print 'Primary Category: %s' % entry.tags[0]['term']\n",
    "    \n",
    "    # Lets get all the categories\n",
    "    all_categories = [t['term'] for t in entry.tags]\n",
    "    print 'All Categories: %s' % (', ').join(all_categories)\n",
    "    \n",
    "    # The abstract is in the <summary> element\n",
    "    print 'Abstract: %s' %  entry.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('play.ipynb'))\n",
    "db_path = os.path.join(BASE_DIR, 'arxiv.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\flame\\\\Documents\\\\Projects\\\\arxiv-oai-scraper\\\\src\\\\arxiv.db'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('play.ipynb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('papers',)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect(db_path)\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71fdff05b81c8e1edf83c38635acb8fe674a3f8cdb8522ca6c810a60cf757fde"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
